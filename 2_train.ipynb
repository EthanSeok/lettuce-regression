{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1317,"status":"ok","timestamp":1653456320164,"user":{"displayName":"­정영준 / 학생 / 생태조경·지역시스템공학부","userId":"12393932611512372244"},"user_tz":-540},"id":"rlhSBOYg2SED"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import os\n","import natsort\n","import copy\n","import torch\n","import pandas as pd\n","import json\n","\n","from torchvision import transforms, models\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import KFold\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2423,"status":"ok","timestamp":1653456322585,"user":{"displayName":"­정영준 / 학생 / 생태조경·지역시스템공학부","userId":"12393932611512372244"},"user_tz":-540},"id":"xy0nUjtF4nt_","outputId":"246165a2-85fe-467d-834b-ad6a2d1b3fbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}],"source":["from google.colab import drive \n","drive.mount('/content/gdrive/')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1653456322585,"user":{"displayName":"­정영준 / 학생 / 생태조경·지역시스템공학부","userId":"12393932611512372244"},"user_tz":-540},"id":"AZNL4yQF6uj1"},"outputs":[],"source":["from google.colab.patches import cv2_imshow"]},{"cell_type":"markdown","metadata":{"id":"UCfsn46J2uzV"},"source":["Data augmentation"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1653456980926,"user":{"displayName":"­정영준 / 학생 / 생태조경·지역시스템공학부","userId":"12393932611512372244"},"user_tz":-540},"id":"60SoD1D_2q8m"},"outputs":[],"source":["def augment(roi, fname, size):\n","    path = '/content/gdrive/My Drive/Colab Notebooks/PartA/data/augment/'\n","    if not os.path.exists(path):\n","        os.mkdir(path)\n","    augmented_images = []\n","    augmented_fnames = []\n","    for n in range(size):\n","        for src, f in zip(roi, fname):\n","            p = np.random.randint(4)\n","            angle = int(90 * p)\n","            h, w = src.shape[:2]\n","            rotation = cv2.getRotationMatrix2D((w/2, h/2), angle, 1)\n","            dst = cv2.warpAffine(src, rotation, (w, h), borderValue=[0, 0, 0, 0])\n","            q = np.random.rand(1)\n","            if q < 0.25:\n","                dst = cv2.flip(dst, 1)\n","            elif q < 0.50:\n","                dst = cv2.flip(dst, 0)\n","            elif q < 0.75:\n","                dst = cv2.flip(dst, 1)\n","                dst = cv2.flip(dst, 0)\n","            f = 'a{}'.format(n) + f\n","            augmented_images.append(dst)\n","            augmented_fnames.append(f)\n","            cv2.imwrite(os.path.join(path, f), dst)\n","            \n","    return augmented_images, augmented_fnames"]},{"cell_type":"markdown","metadata":{"id":"YT002bQb4bGy"},"source":["Prepare data - Extract ROI (Region of interest)"]},{"cell_type":"code","execution_count":94,"metadata":{"executionInfo":{"elapsed":312,"status":"ok","timestamp":1653458471428,"user":{"displayName":"­정영준 / 학생 / 생태조경·지역시스템공학부","userId":"12393932611512372244"},"user_tz":-540},"id":"03X9pHu03ovA"},"outputs":[],"source":["def prepare_data(mode='train', augmentation_size=8):\n","    if mode == 'train':\n","        p = '/content/gdrive/My Drive/Colab Notebooks/PartA/data/'\n","        needAugment = True\n","        verbose = True\n","    elif mode == 'evaluate':\n","        p = '/content/gdrive/My Drive/Colab Notebooks/PartA/eval/'\n","        needAugment = False\n","        verbose = False\n","\n","    input_rgb_path = []\n","    input_depth_path = []\n","    input_rgb_fname = []\n","    input_depth_fname = []\n","    lettuce_rgb_roi = []\n","    lettuce_depth_roi = []\n","\n","    save_path = os.path.join(p, 'ROI')\n","    \n","    ROI_SIZE = (768, 768)\n","\n","    for path, direct, files in os.walk(p):\n","        for f in files:\n","            if f.startswith('RGB_') and not path.endswith('ROI'):\n","                input_rgb_fname.append(f)\n","                input_rgb_path.append(os.path.join(path, f))\n","            elif f.startswith('Debth_') and not path.endswith('ROI'):\n","                input_depth_fname.append(f)\n","                input_depth_path.append(os.path.join(path, f))\n","\n","    if verbose:\n","        print(\"Got {} RGB images\".format(len(input_rgb_path)))\n","        print(\"Got {} depth images\".format(len(input_depth_path)))\n","\n","    rgb_idx = natsort.index_natsorted(input_rgb_fname)\n","    depth_idx = natsort.index_natsorted(input_depth_fname)\n","\n","    input_rgb_path = natsort.order_by_index(input_rgb_path, rgb_idx)\n","    input_rgb_fname = natsort.order_by_index(input_rgb_fname, rgb_idx)\n","    input_depth_path = natsort.order_by_index(input_depth_path, depth_idx)\n","    input_depth_fname = natsort.order_by_index(input_depth_fname, depth_idx)\n","\n","    for fname, fname_d, savename, savename_d in zip(input_rgb_path, input_depth_path, input_rgb_fname, input_depth_fname):\n","        # Get RGB image and depth image simultaneously\n","        src = cv2.imread(fname)\n","        src_d = cv2.imread(fname_d, 3)\n","        # Take 1 channel of the depth image\n","        src_d = cv2.cvtColor(src_d, cv2.COLOR_BGR2GRAY)\n","        src_h, src_w = src.shape[:2]\n","        _, src_d = cv2.threshold(src_d, 1000, 255, cv2.THRESH_TOZERO_INV)\n","   \n","        src_d = np.interp(src_d, [0, src_d.max()], [0, 255]).astype('uint8')\n","        center_h, center_w = src_h // 2, src_w // 2\n","        roi = src[center_h  - ROI_SIZE[1] // 2: center_h + ROI_SIZE[1] // 2, center_w - ROI_SIZE[0] // 2: center_w + ROI_SIZE[0] // 2]\n","        roi_d = src_d[center_h  - ROI_SIZE[1] // 2: center_h + ROI_SIZE[1] // 2, center_w - ROI_SIZE[0] // 2: center_w + ROI_SIZE[0] // 2]\n","        roi_d = np.interp(roi_d, [0, roi_d.max()], [0, 255]).astype('uint8')\n","        roi_d = roi_d.reshape((ROI_SIZE[0], ROI_SIZE[1], 1))\n","        \n","    \n","        roi_m = np.concatenate((roi, roi_d), axis=2)\n","        # roi_m = roi\n","        roi_m = cv2.resize(roi_m, (256, 256))\n","\n","        num = int(fname.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1])\n","        lettuce_rgb_roi.append(roi_m)\n","        # print(roi.shape)\n","        savename = os.path.join(save_path, savename)\n","\n","        cv2.imwrite(savename, roi_m)\n","    \n","    lettuce_rgb_fnames = copy.copy(input_rgb_fname)\n","\n","    \n","    \n","\n","    if needAugment:\n","        augmented_images, augmented_fnames = augment(lettuce_rgb_roi, lettuce_rgb_fnames, augmentation_size)\n","        lettuce_rgb_roi.extend(augmented_images)\n","        lettuce_rgb_fnames.extend(augmented_fnames)\n","\n","    if verbose:\n","        print(\"Image prepared! \")\n","        print(\"Processed {} images\".format(len(lettuce_rgb_roi)))\n","        # to show original images\n","        cv2_imshow(src)\n","        cv2_imshow(roi)\n","        cv2_imshow(roi_m)\n","\n","    return lettuce_rgb_roi, lettuce_rgb_fnames"]},{"cell_type":"code","execution_count":109,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Cj3aoVfBKR-AS20jnRA3GwPnrbXOKSSM"},"executionInfo":{"elapsed":51663,"status":"ok","timestamp":1653458669832,"user":{"displayName":"­정영준 / 학생 / 생태조경·지역시스템공학부","userId":"12393932611512372244"},"user_tz":-540},"id":"W244rgmd4S7U","outputId":"f399d634-a563-4666-ea63-f8924e91f983"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["input_images, input_fnames = prepare_data(augmentation_size=8)\n","test_images, test_fnames = prepare_data(mode='evaluate')\n"]},{"cell_type":"markdown","source":["Define custom dataset"],"metadata":{"id":"JftkJcpd9L2d"}},{"cell_type":"code","source":["IMG_SIZE = (256, 256)\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, image, label, transform=None):\n","        self.transform = transform\n","        self.image = image\n","        self.label = label\n","        self.img_size = (256, 256)\n","\n","\n","    def __len__(self):\n","        return len(self.image)\n","\n","    def __getitem__(self, index):\n","        self.img_data = self.image[index][0]\n","        self.img_label = self.label[index]\n","        self.img_fname = self.image[index][1]\n","\n","        return [self.img_data, self.img_label, self.img_fname]\n","\n","    def custom_collate_fn(self, data):\n","        inputImages = []\n","        outputVectors = []\n","        fileNames = []\n","        h, w = self.img_size\n","        for sample in data:\n","            img = sample[0]\n","            label = sample[1]\n","            fname = sample[2]\n","\n","            if img.ndim == 2:\n","                img = img[:, :, np.newaxis]\n","            \n","            inputImages.append(img.reshape((h, w, 4)).transpose((2, 0, 1)).astype(np.float32))\n","            outputVectors.append(label)\n","            fileNames.append(fname)\n","\n","        data = {'input': inputImages, 'label': outputVectors, 'fname': fileNames}\n","        \n","        return data\n","\n","\n","class ToTensor(object):\n","  def __call__(self, data):\n","    label, input, fname = data['label'], data['input'], data['fname']\n","    h, w = IMG_SIZE\n","    input_tensor = torch.empty(len(input), 4, h, w)\n","    label_tensor = torch.empty(len(input), 5)\n","    for i in range(len(input)):\n","      input[i] = input[i].transpose((2, 0, 1)).astype(np.float32)\n","      input_tensor[i] = torch.from_numpy(input[i])\n","      label_tensor[i] = torch.from_numpy(label[i])\n","\n","    data = {'label': label_tensor, 'input': input_tensor}\n","\n","    return data\n"],"metadata":{"id":"RxIuADGy9Npr","executionInfo":{"status":"ok","timestamp":1653458669832,"user_tz":-540,"elapsed":3,"user":{"displayName":"­정영준 / 학생 / 생태조경·지역시스템공학부","userId":"12393932611512372244"}}},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":["Load data"],"metadata":{"id":"xRHu2ikIOP87"}},{"cell_type":"code","source":["input_images = np.array(input_images)\n","input_fnames = np.array(input_fnames)\n","\n","target_df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/PartA/results/output_df.csv', index_col='Unnamed: 0')\n","target_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"ooGCOBZN9Y-b","executionInfo":{"status":"ok","timestamp":1653458670379,"user_tz":-540,"elapsed":549,"user":{"displayName":"­정영준 / 학생 / 생태조경·지역시스템공학부","userId":"12393932611512372244"}},"outputId":"d629d6b8-9183-4448-8ab3-5a9894a467d9"},"execution_count":111,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          shoot_fw  shoot_dw  height  diameter  leafarea\n","Image27        5.5      0.42     9.0      11.0     153.9\n","Image79       30.3      1.92     8.8      20.2     582.3\n","Image140      60.9      2.83    11.6      19.0     960.6\n","Image203     112.0      5.76    11.0      22.0    1614.9\n","Image292     372.6     14.17    17.0      32.0    3839.6\n","...            ...       ...     ...       ...       ...\n","Image20        4.5      0.40     8.5      13.0     127.4\n","Image31        6.6      0.58     7.5      14.6     159.3\n","Image197     127.0      6.30    12.0      21.0    2110.5\n","Image246     224.1      8.61    15.0      24.5    3777.1\n","Image304     206.8      7.99    18.0      24.5    2604.8\n","\n","[261 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-68eb77ac-0b98-416f-bdf6-f50b43e82e08\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>shoot_fw</th>\n","      <th>shoot_dw</th>\n","      <th>height</th>\n","      <th>diameter</th>\n","      <th>leafarea</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Image27</th>\n","      <td>5.5</td>\n","      <td>0.42</td>\n","      <td>9.0</td>\n","      <td>11.0</td>\n","      <td>153.9</td>\n","    </tr>\n","    <tr>\n","      <th>Image79</th>\n","      <td>30.3</td>\n","      <td>1.92</td>\n","      <td>8.8</td>\n","      <td>20.2</td>\n","      <td>582.3</td>\n","    </tr>\n","    <tr>\n","      <th>Image140</th>\n","      <td>60.9</td>\n","      <td>2.83</td>\n","      <td>11.6</td>\n","      <td>19.0</td>\n","      <td>960.6</td>\n","    </tr>\n","    <tr>\n","      <th>Image203</th>\n","      <td>112.0</td>\n","      <td>5.76</td>\n","      <td>11.0</td>\n","      <td>22.0</td>\n","      <td>1614.9</td>\n","    </tr>\n","    <tr>\n","      <th>Image292</th>\n","      <td>372.6</td>\n","      <td>14.17</td>\n","      <td>17.0</td>\n","      <td>32.0</td>\n","      <td>3839.6</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>Image20</th>\n","      <td>4.5</td>\n","      <td>0.40</td>\n","      <td>8.5</td>\n","      <td>13.0</td>\n","      <td>127.4</td>\n","    </tr>\n","    <tr>\n","      <th>Image31</th>\n","      <td>6.6</td>\n","      <td>0.58</td>\n","      <td>7.5</td>\n","      <td>14.6</td>\n","      <td>159.3</td>\n","    </tr>\n","    <tr>\n","      <th>Image197</th>\n","      <td>127.0</td>\n","      <td>6.30</td>\n","      <td>12.0</td>\n","      <td>21.0</td>\n","      <td>2110.5</td>\n","    </tr>\n","    <tr>\n","      <th>Image246</th>\n","      <td>224.1</td>\n","      <td>8.61</td>\n","      <td>15.0</td>\n","      <td>24.5</td>\n","      <td>3777.1</td>\n","    </tr>\n","    <tr>\n","      <th>Image304</th>\n","      <td>206.8</td>\n","      <td>7.99</td>\n","      <td>18.0</td>\n","      <td>24.5</td>\n","      <td>2604.8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>261 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68eb77ac-0b98-416f-bdf6-f50b43e82e08')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-68eb77ac-0b98-416f-bdf6-f50b43e82e08 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-68eb77ac-0b98-416f-bdf6-f50b43e82e08');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":111}]},{"cell_type":"markdown","source":["Normalize data to 0~1"],"metadata":{"id":"yLOUmQvCOPiD"}},{"cell_type":"code","source":["max_values = target_df.max().values\n","min_values = target_df.min().values\n","target_df = (target_df - min_values) / (max_values - min_values)"],"metadata":{"id":"qMr6xr5BOLEV","executionInfo":{"status":"ok","timestamp":1653458670380,"user_tz":-540,"elapsed":6,"user":{"displayName":"­정영준 / 학생 / 생태조경·지역시스템공학부","userId":"12393932611512372244"}}},"execution_count":112,"outputs":[]},{"cell_type":"markdown","source":["Extract images in target_df"],"metadata":{"id":"WMHpZ8NVOuLa"}},{"cell_type":"code","source":["img_idx = ['Image'+ s.split(\"_\")[1].split(\".\")[0] for s in input_fnames]\n","target_df = target_df.loc[img_idx, :]"],"metadata":{"id":"CfK_rF8COmCt","executionInfo":{"status":"ok","timestamp":1653458670380,"user_tz":-540,"elapsed":5,"user":{"displayName":"­정영준 / 학생 / 생태조경·지역시스템공학부","userId":"12393932611512372244"}}},"execution_count":113,"outputs":[]},{"cell_type":"markdown","source":["Process input images and output labels"],"metadata":{"id":"AH62BmjfU9tG"}},{"cell_type":"code","source":["idx = natsort.index_natsorted(input_fnames)\n","input_fnames = np.array(natsort.order_by_index(input_fnames, idx))\n","input_images = np.array(natsort.order_by_index(input_images, idx))\n","output_labels = target_df.values\n","\n","transform = transforms.Compose([ToTensor()])"],"metadata":{"id":"VfO8ebqROs1c","executionInfo":{"status":"ok","timestamp":1653458670380,"user_tz":-540,"elapsed":5,"user":{"displayName":"­정영준 / 학생 / 생태조경·지역시스템공학부","userId":"12393932611512372244"}}},"execution_count":114,"outputs":[]},{"cell_type":"code","source":["batch_size = 64\n","epochs = 20\n","lr = 0.0001\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Current device:', device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UWQvn_x9Y8XL","executionInfo":{"status":"ok","timestamp":1653458670380,"user_tz":-540,"elapsed":5,"user":{"displayName":"­정영준 / 학생 / 생태조경·지역시스템공학부","userId":"12393932611512372244"}},"outputId":"4356f5db-9d8b-41b1-e998-d2638d6e3cda"},"execution_count":115,"outputs":[{"output_type":"stream","name":"stdout","text":["Current device: cuda\n"]}]},{"cell_type":"code","source":["kf = KFold(5)\n","kf\n","\n","fold_idx = 0"],"metadata":{"id":"fUltC3JuUYDB","executionInfo":{"status":"ok","timestamp":1653458670381,"user_tz":-540,"elapsed":4,"user":{"displayName":"­정영준 / 학생 / 생태조경·지역시스템공학부","userId":"12393932611512372244"}}},"execution_count":116,"outputs":[]},{"cell_type":"code","source":["for train_idx, val_idx in kf.split(output_labels):\n","  fold_idx += 1\n","  image_train, image_val = input_images[train_idx], input_images[val_idx]\n","  label_train, label_val = output_labels[train_idx], output_labels[val_idx]\n","\n","  fname_train, fname_val = input_fnames[train_idx], input_fnames[val_idx]\n","\n","  image_train = list(zip(image_train, fname_train))\n","  image_val = list(zip(image_val, fname_val))\n","\n","\n","  earlyStoppingCount = 0\n","  dataset_train = CustomDataset(image_train, label_train)\n","  loader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, collate_fn=dataset_train.custom_collate_fn, num_workers=0)\n","  dataset_val = CustomDataset(image_val, label_val)\n","  loader_val = DataLoader(dataset_val, batch_size=len(val_idx), shuffle=True, collate_fn=dataset_val.custom_collate_fn, num_workers=0)  \n","\n","  # define model\n","  model = models.resnet18(pretrained=True)\n","  model.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","  model.fc = nn.Linear(512, 5, bias=True)\n","\n","  model.to(device)\n","  criterion = nn.MSELoss().to(device)\n","  optim = torch.optim.Adam(model.parameters(), lr=lr)\n","  best_epoch = 0\n","  val_loss_save = np.inf\n","\n","  for n, epoch in enumerate(range(epochs)):\n","    model.train()\n","    train_loss = []\n","\n","    for batch, data in enumerate(loader_train, 1):\n","      label = torch.tensor(data['label'], dtype=torch.float32).to(device)\n","      input = torch.tensor(data['input'], dtype=torch.float32).to(device)\n","      output = model(input)\n","      \n","      optim.zero_grad()\n","\n","      loss = criterion(output, label)\n","      loss.backward()\n","\n","      optim.step()\n","\n","      train_loss += [loss.item()]\n","    \n","    with torch.no_grad():\n","      model.eval()\n","      val_loss = []\n","      for batch, data in enumerate(loader_val, 1):\n","        label_val = torch.tensor(data['label'], dtype=torch.float32).to(device)\n","        input_val = torch.tensor(data['input'], dtype=torch.float32).to(device)\n","        output_val = model(input_val)\n","        loss = criterion(output_val, label_val)\n","        val_loss += [loss.item()] \n","      \n","      val_loss_tmp = np.mean(val_loss)\n","      earlyStoppingCount += 1\n","      \n","      if val_loss_tmp < val_loss_save:\n","          earlyStoppingCount = 0\n","          best_epoch = epoch\n","          val_loss_save = val_loss_tmp.item()\n","          torch.save(model.state_dict(), '/content/gdrive/My Drive/Colab Notebooks/PartA/models/param{}.data'.format(fold_idx))\n","          print(\".......model updated (epoch = \", epoch+1, \")\")\n","      print(\"epoch: %04d / %04d | train loss: %.5f | validation loss: %.5f\" % (epoch+1, epochs, np.mean(train_loss), np.mean(val_loss)))\n","    \n","  print(\"Model with the best validation accuracy is saved.\")\n","  print(\"Best epoch: \", best_epoch)\n","  print(\"Best validation loss: {:.5f}\".format(val_loss_save))\n","  print(\"Done.\")\n","\n","  if fold_idx > 0:\n","    break\n","   "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IhwsAGyuUanr","outputId":"afb77bd4-c82e-4b39-a7d4-c5b53f56f475"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[".......model updated (epoch =  1 )\n","epoch: 0001 / 0020 | train loss: 0.06235 | validation loss: 0.03498\n",".......model updated (epoch =  2 )\n","epoch: 0002 / 0020 | train loss: 0.01374 | validation loss: 0.01044\n",".......model updated (epoch =  3 )\n","epoch: 0003 / 0020 | train loss: 0.00618 | validation loss: 0.00886\n",".......model updated (epoch =  4 )\n","epoch: 0004 / 0020 | train loss: 0.00359 | validation loss: 0.00593\n","epoch: 0005 / 0020 | train loss: 0.00282 | validation loss: 0.00609\n"]}]},{"cell_type":"code","source":["image_test = test_images\n","fname_test = test_fnames\n","\n","# cv2_imshow(image_test[0])\n","image_t = np.array(image_test).transpose((0, 3, 1, 2))\n","input_test = torch.tensor(image_t, dtype=torch.float32).to(device)\n","\n","res = model(input_test).cpu().detach().numpy()\n","res = (max_values - min_values) * res + min_values\n","res\n","\n"],"metadata":{"id":"JOXL9wdyYZep"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FILE_DIRECTORY = '/content/gdrive/My Drive/Colab Notebooks/PartA/eval/GroundTruth.json'\n","with open(FILE_DIRECTORY) as data_file:    \n","    JSON_data = json.load(data_file)\n","image_index = np.array(list(JSON_data['Measurements'].keys()))\n","shoot_fw = []\n","shoot_dw = []\n","height = []\n","diameter = []\n","leafarea = []\n","\n","img_idx = ['Image'+ s.split(\"_\")[1].split(\".\")[0] for s in fname_test]\n","\n","for _ in JSON_data['Measurements'].keys():\n","    shoot_fw.append(JSON_data['Measurements'][_]['FreshWeightShoot'])\n","    shoot_dw.append(JSON_data['Measurements'][_]['DryWeightShoot'])\n","    height.append(JSON_data['Measurements'][_]['Height'])\n","    diameter.append(JSON_data['Measurements'][_]['Diameter'])\n","    leafarea.append(JSON_data['Measurements'][_]['LeafArea'])\n","output_df = pd.DataFrame(np.array([shoot_fw, shoot_dw, height, diameter, leafarea]).T, index=image_index, columns=['shoot_fw', 'shoot_dw', 'height', 'diameter', 'leafarea'])\n","\n","output_df = output_df.loc[img_idx, :]\n"],"metadata":{"id":"y49suoJVlj8D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_df"],"metadata":{"id":"AOHAvZQjlDiV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","res_df = pd.DataFrame(res, columns=['shoot_fw', 'shoot_dw', 'height', 'diameter', 'leafarea'], index=img_idx)\n","res_df"],"metadata":{"id":"sDpL15XUmIY-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n1 = (output_df - res_df) ** 2\n","n2 = output_df ** 2\n","nmse = np.sum(n1) / np.sum(n2)\n","print(nmse)\n","print(np.sum(nmse))"],"metadata":{"id":"snqpiBKNtRWL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ZpSjrWZ3zXxJ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"2_train.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOASe658XeYuCnlERH2uPkA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}